{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 11:03:48.864454: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 11:03:48.895943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'audioclas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudioclas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m paths, config, utils, model_utils\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudioclas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data_splits, load_class_names, data_sequence, generate_embeddings,\\\n\u001b[1;32m     43\u001b[0m     file_to_PCM_16bits, json_friendly, save_embeddings_txt\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudioclas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelWrapper\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'audioclas'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training runfile\n",
    "\n",
    "Date: October 2019\n",
    "Author: Ignacio Heredia\n",
    "Email: iheredia@ifca.unican.es\n",
    "Github: ignacioheredia\n",
    "\n",
    "Description:\n",
    "This file contains the commands for training an audio classifier.\n",
    "\n",
    "Additional notes:\n",
    "* On the training routine: Preliminary tests show that using a custom lr multiplier for the lower layers yield to better\n",
    "results than freezing them at the beginning and unfreezing them after a few epochs like it is suggested in the Keras\n",
    "tutorials.\n",
    "\"\"\"\n",
    "\n",
    "#TODO List:\n",
    "\n",
    "#TODO: Implement resuming training\n",
    "#TODO: Try that everything works out with validation data\n",
    "#TODO: Try several regularization parameters\n",
    "#TODO: Add additional metrics for test time in addition to accuracy\n",
    "#TODO: Implement additional techniques to deal with class imbalance (not only class_weigths)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from audioclas import paths, config, utils, model_utils\n",
    "from audioclas.data_utils import load_data_splits, load_class_names, data_sequence, generate_embeddings,\\\n",
    "    file_to_PCM_16bits, json_friendly, save_embeddings_txt\n",
    "from audioclas.model import ModelWrapper\n",
    "from audioclas.optimizers import customAdam\n",
    "\n",
    "# Set Tensorflow verbosity logs\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Dynamically grow the memory used on the GPU (https://github.com/keras-team/keras/issues/4161)\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "tfconfig = tf.ConfigProto(gpu_options=gpu_options)\n",
    "sess = tf.Session(config=tfconfig)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "CONF = config.get_conf_dict()\n",
    "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "CONF[\"general\"][\"dataset_directory\"]='/storage/Imagine_UC6/data_new_ais/data50_extra'\n",
    "\n",
    "CONF[\"training\"][\"use_early_stopping\"]=True\n",
    "CONF[\"training\"][\"batch_size\"]=100\n",
    "CONF[\"training\"][\"epochs\"]=15\n",
    "\n",
    "CONF[\"preprocessing\"][\"files_to_PCM\"]=True\n",
    "CONF[\"preprocessing\"][\"compute_embeddings\"]=True\n",
    "\n",
    "paths.timestamp = TIMESTAMP\n",
    "paths.CONF = CONF\n",
    "\n",
    "utils.create_dir_tree()\n",
    "utils.backup_splits()\n",
    "load_class_names(splits_dir=paths.get_ts_splits_dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF[\"training\"][\"batch_size\"]=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n",
      "new version\n",
      "Loading val data...\n",
      "new version\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "If you do not compute the embeddings, your train/val.txt should point to theembeddings `.npy` files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f6c8163e7903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         raise Exception(\"If you do not compute the embeddings, your train/val.txt should point to the\"\n\u001b[0m\u001b[1;32m     65\u001b[0m                         \"embeddings `.npy` files.\")\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: If you do not compute the embeddings, your train/val.txt should point to theembeddings `.npy` files."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "X_train, y_train = load_data_splits(splits_dir=paths.get_ts_splits_dir(),\n",
    "                                    dataset_dir=paths.get_dataset_dir(),\n",
    "                                    split_name='train')\n",
    "\n",
    "# Load the validation data\n",
    "if (CONF['training']['use_validation']) and ('val.txt' in os.listdir(paths.get_ts_splits_dir())):\n",
    "    X_val, y_val = load_data_splits(splits_dir=paths.get_ts_splits_dir(),\n",
    "                                    dataset_dir=paths.get_dataset_dir(),\n",
    "                                    split_name='val')\n",
    "else:\n",
    "    print('No validation data.')\n",
    "    X_val, y_val = None, None\n",
    "    CONF['training']['use_validation'] = False\n",
    "\n",
    "# Load the class names\n",
    "class_names = 1 #load_class_names(splits_dir=paths.get_ts_splits_dir())\n",
    "\n",
    "# Update the configuration\n",
    "CONF['training']['batch_size'] = min(CONF['training']['batch_size'], len(X_train))\n",
    "\n",
    "if CONF['model']['num_classes'] is None:\n",
    "    CONF['model']['num_classes'] = 1#len(class_names)\n",
    "\n",
    "# assert CONF['model']['num_classes'] >= np.amax(y_train),\\\n",
    "#     \"Your train.txt file has more categories than those defined in classes.txt\"\n",
    "# if CONF['training']['use_validation']:\n",
    "#     assert CONF['model']['num_classes'] >= np.amax(y_val),\\\n",
    "#         \"Your val.txt file has more categories than those defined in classes.txt\"\n",
    "\n",
    "# Transform the training data to scipy-compatible 16-bit\n",
    "if not CONF['preprocessing']['compute_embeddings']:  # no need to compute if embeddings are precomputed\n",
    "    CONF['preprocessing']['files_to_PCM'] = False\n",
    "\n",
    "if CONF['preprocessing']['files_to_PCM']:\n",
    "    print('Transforming inputs to PCM 16-bits ...')\n",
    "    for p in tqdm(X_train):\n",
    "        file_to_PCM_16bits(p)\n",
    "    if CONF['training']['use_validation']:\n",
    "        for p in tqdm(X_val):\n",
    "            file_to_PCM_16bits(p)\n",
    "\n",
    "# Create model wrapper\n",
    "model_wrap = ModelWrapper(classifier_model=os.path.join(paths.get_models_dir(), 'audioset', 'ckpts',\n",
    "                                                        'final_model.h5'))\n",
    "\n",
    "# Generating new embeddings if needed\n",
    "if CONF['preprocessing']['compute_embeddings']:\n",
    "    print('Clearing old embeddings ...')\n",
    "    embed_dir = paths.get_embeddings_dir()\n",
    "#     for f in os.listdir(embed_dir):\n",
    "#         os.remove(os.path.join(embed_dir, f))\n",
    "\n",
    "    print(\"Generating new embeddings ...\")\n",
    "    X_train, y_train = generate_embeddings(model_wrap=model_wrap, filepaths=X_train, labels=y_train)\n",
    "    save_embeddings_txt(X_train, y_train, 'train_emb.txt')\n",
    "\n",
    "    if CONF['training']['use_validation']:\n",
    "        X_val, y_val = generate_embeddings(model_wrap=model_wrap, filepaths=X_val, labels=y_val)\n",
    "        save_embeddings_txt(X_val, y_val, 'val_emb.txt')\n",
    "\n",
    "else:\n",
    "    if not X_train[0].endswith('.npy'):\n",
    "        raise Exception(\"If you do not compute the embeddings, your train/val.txt should point to the\"\n",
    "                        \"embeddings `.npy` files.\")\n",
    "\n",
    "# Create data generator for train and val sets\n",
    "train_gen = data_sequence(X_train, y_train,\n",
    "                          batch_size=CONF['training']['batch_size'],\n",
    "                          num_classes=CONF['model']['num_classes'], shuffle=False)\n",
    "train_steps = int(np.ceil(len(X_train)/CONF['training']['batch_size']))\n",
    "\n",
    "if CONF['training']['use_validation']:\n",
    "    val_gen = data_sequence(X_val, y_val,\n",
    "                            batch_size=CONF['training']['batch_size'],\n",
    "                            num_classes=CONF['model']['num_classes'], shuffle=False)\n",
    "    val_steps = int(np.ceil(len(X_val)/CONF['training']['batch_size']))\n",
    "else:\n",
    "    val_gen = None\n",
    "    val_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from matplotlib.cm import get_cmap\n",
    "class LinePlotCallback(Callback):\n",
    "    def __init__(self, train_gen, val_gen, val_steps, class_names):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.val_steps = val_steps\n",
    "        self.class_names = class_names\n",
    "        self.epoch_count = 0\n",
    "        self.colors = get_cmap('tab20').colors\n",
    "        self.wout_avg_train = []\n",
    "        self.wout_avg_val = []\n",
    "        self.abs_diff_history_train = []\n",
    "        self.abs_diff_history_val = []\n",
    "        \n",
    "        \n",
    "        self.predicted_values_train=[]\n",
    "        self.predicted_values_val=[]\n",
    "        \n",
    "        self.real_values_val=[]\n",
    "        self.real_values_train=[]\n",
    "\n",
    "    def save_predicted_values(self, generator, predicted_values_list, real_values_list, abs_diff_history_list,datapoints):\n",
    "        predicted_values_epoch = []\n",
    "        real_values_epoch = []\n",
    "\n",
    "        for batch in range(len(generator)):\n",
    "            batch_X, batch_y = generator[batch]\n",
    "            batch_y_pred = self.model.predict(batch_X)\n",
    "            predicted_values_epoch.append(batch_y_pred[0:datapoints])\n",
    "            real_values_epoch.append(batch_y[0:datapoints])\n",
    "\n",
    "            abs_diff = np.abs(batch_y - batch_y_pred)\n",
    "            avg_abs_diff = np.mean(abs_diff)#, axis=0)\n",
    "            abs_diff_history_list.append(avg_abs_diff)\n",
    "\n",
    "        predicted_values_list.append(predicted_values_epoch)\n",
    "        real_values_list.append(real_values_epoch)\n",
    "        return predicted_values_list, real_values_list\n",
    "\n",
    "    def transform_values(self,values):\n",
    "            return [(10000 - 10000 * y) for y in values]\n",
    "        \n",
    "    def plot_batch_predicted_and_real(self, batch, predicted_values_list, real_values_list, save_dir, title, datapoints, is_validation=False):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        # Plot real values only once for each batch\n",
    "        real_values = real_values_list[0][batch]\n",
    "        names=[]\n",
    "        for i in range(datapoints):\n",
    "            names.append(f\"dp {i}\")\n",
    "\n",
    "        real_values = self.transform_values(real_values)\n",
    "        predicted_values_list = [self.transform_values(epoch_values) for epoch_values in predicted_values_list]\n",
    "\n",
    "\n",
    "        plt.scatter(names, real_values, label='Real', alpha=0.7, color='black', marker='o', s=100)\n",
    "\n",
    "        num_epochs = len(predicted_values_list)\n",
    "\n",
    "        for epoch, predicted_values_epoch in enumerate(predicted_values_list):\n",
    "            predicted_values = predicted_values_epoch[batch]\n",
    "            color = self.colors[epoch % len(self.colors)]\n",
    "#             marker = self.markers[epoch % len(self.markers)]\n",
    "            alpha = 1-(1.0 - (epoch / (num_epochs)) )\n",
    "            alpha= (epoch + 1) / num_epochs \n",
    "            plt.scatter(names, predicted_values, label=f'Epoch {epoch + 1}, Predicted', alpha=alpha, color=color)\n",
    "\n",
    "        plt.ylim(0, 10000)\n",
    "        plt.xlabel('Class Names')\n",
    "        plt.ylabel('Values')\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "\n",
    "        if is_validation:\n",
    "            plt.savefig(os.path.join(save_dir, f'validation_batch_{batch}.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(save_dir, f'training_batch_{batch}.png'))\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_values_for_iterations(self, wout_avg, abs_diff_history, save_dir, title, is_validation=False):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        num_iterations = len(wout_avg)\n",
    "        num_classes = len(self.class_names)\n",
    "\n",
    "        # Create a DataFrame from the list\n",
    "        iterations = [f'Iteration {i + 1}' for i in range(num_iterations)]\n",
    "\n",
    "        for i in range(len(self.class_names)):\n",
    "            plt.plot(range(num_iterations), [wout_avg[j][i] for j in range(num_iterations)], label=self.class_names[i])\n",
    "\n",
    "        # Set x-tick labels\n",
    "        plt.xticks(range(num_iterations), iterations)\n",
    "\n",
    "        # Add a legend\n",
    "        plt.legend()\n",
    "\n",
    "        # Set labels and title\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Values')\n",
    "        plt.title(title)\n",
    "\n",
    "        if is_validation:\n",
    "            plt.savefig(os.path.join(save_dir, f'{title}_validation.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(save_dir, f'{title}_training.png'))\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_count = epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Save predicted values for training and validation\n",
    "#         predicted_values_train = []\n",
    "#         real_values_train = []\n",
    "        abs_diff_history_train = []\n",
    "#         predicted_values_val = []\n",
    "#         real_values_val = []\n",
    "        abs_diff_history_val = []\n",
    "        datapoints=10\n",
    "        self.predicted_values_train,self.real_values_train= self.save_predicted_values(self.train_gen, self.predicted_values_train, self.real_values_train, abs_diff_history_train,datapoints)\n",
    "        self.predicted_values_val,self.real_values_val=self.save_predicted_values(self.val_gen, self.predicted_values_val, self.real_values_val, abs_diff_history_val,datapoints)\n",
    "\n",
    "        # Plot batch predicted values and real values for training and validation\n",
    "        save_dir = paths.get_plots_dir()  # Replace with your actual directory path\n",
    "\n",
    "        for batch in range(len(self.predicted_values_train[0])):\n",
    "            self.plot_batch_predicted_and_real(\n",
    "                batch, self.predicted_values_train, self.real_values_train,\n",
    "                save_dir, f'Batch {batch} Predicted and Real Values for Training', datapoints,is_validation=False\n",
    "            )\n",
    "\n",
    "        for batch in range(len(self.predicted_values_val[0])):\n",
    "            self.plot_batch_predicted_and_real(\n",
    "                batch, self.predicted_values_val, self.real_values_val,\n",
    "                save_dir, f'Batch {batch} Predicted and Real Values for Validation',datapoints, is_validation=True\n",
    "            )\n",
    "\n",
    "#         # Transpose the abs_diff_history to get per-label data\n",
    "#         avg_abs_diff_history_train = np.array(abs_diff_history_train).T\n",
    "#         avg_abs_diff_history_val = np.array(abs_diff_history_val).T\n",
    "\n",
    "#         self.wout_avg_train.append(np.mean(avg_abs_diff_history_train))\n",
    "#         self.wout_avg_val.append(np.mean(avg_abs_diff_history_val))\n",
    "\n",
    "#         # Plot values for different iterations\n",
    "#         self.plot_values_for_iterations(\n",
    "#             self.wout_avg_train, abs_diff_history_train,\n",
    "#             save_dir, 'Values for Different Iterations for Training', is_validation=False\n",
    "#         )\n",
    "\n",
    "#         self.plot_values_for_iterations(\n",
    "#             self.wout_avg_val, abs_diff_history_val,\n",
    "#             save_dir, 'Values for Different Iterations for Validation', is_validation=True\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-73b092e30afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mline_plot_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinePlotCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Initialize the callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Include the custom callback in your list of callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_gen' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Launch the training\n",
    "t0 = time.time()\n",
    "\n",
    "# Create the model and compile it\n",
    "model, base_model = model_utils.create_model(CONF, base_model=model_wrap.classify_model)\n",
    "\n",
    "# Get a list of the top layer variables that should not be applied a lr_multiplier\n",
    "base_vars = [var.name for var in base_model.trainable_variables]\n",
    "all_vars = [var.name for var in model.trainable_variables]\n",
    "top_vars = set(all_vars) - set(base_vars)\n",
    "top_vars = list(top_vars)\n",
    "\n",
    "# Set trainable layers\n",
    "if CONF['training']['mode'] == 'fast':\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model.compile(optimizer=customAdam(lr=CONF['training']['initial_lr'],\n",
    "                                   amsgrad=True,\n",
    "                                   lr_mult=0.1,\n",
    "                                   excluded_vars=top_vars\n",
    "                                   ),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae', 'mse'])\n",
    "\n",
    "\n",
    "\n",
    "line_plot_callback = LinePlotCallback(train_gen,val_gen, val_steps, class_names)  # Initialize the callback\n",
    "\n",
    "# Include the custom callback in your list of callbacks\n",
    "callbacks = utils.get_callbacks(CONF)\n",
    "# callbacks.append(bar_plot_callback)\n",
    "callbacks.append(line_plot_callback)\n",
    "\n",
    "history = model.fit_generator(generator=train_gen,\n",
    "                              steps_per_epoch=train_steps,\n",
    "                              epochs=CONF['training']['epochs'],\n",
    "                              class_weight=None,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=callbacks,\n",
    "                              verbose=1, max_queue_size=5, workers=4,\n",
    "                              use_multiprocessing=CONF['training']['use_multiprocessing'],\n",
    "                              initial_epoch=0)\n",
    "\n",
    "\n",
    "# Saving everything\n",
    "print('Saving data to {} folder.'.format(paths.get_timestamped_dir()))\n",
    "print('Saving training stats ...')\n",
    "stats = {'epoch': history.epoch,\n",
    "         'training time (s)': round(time.time()-t0, 2),\n",
    "         'timestamp': TIMESTAMP}\n",
    "stats.update(history.history)\n",
    "stats = json_friendly(stats)\n",
    "stats_dir = paths.get_stats_dir()\n",
    "with open(os.path.join(stats_dir, 'stats.json'), 'w') as outfile:\n",
    "    json.dump(stats, outfile, sort_keys=True, indent=4)\n",
    "\n",
    "print('Saving the configuration ...')\n",
    "model_utils.save_conf(CONF)\n",
    "\n",
    "print('Saving the model to h5...')\n",
    "fpath = os.path.join(paths.get_checkpoints_dir(), 'final_model.h5')\n",
    "model.save(fpath)\n",
    "\n",
    "# print('Saving the model to protobuf...')\n",
    "# fpath = os.path.join(paths.get_checkpoints_dir(), 'final_model.proto')\n",
    "# model_utils.save_to_pb(model, fpath)\n",
    "\n",
    "print('Finished')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Plot boxplots for each label\n",
    "\n",
    "plt.boxplot(y_train)\n",
    "plt.title(f'Boxplot for')\n",
    "plt.ylim(-0.1, 1)  # Set y-axis limit from 0 to 1\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.tight_layout()  # Ensure proper spacing\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Plot boxplots for each label\n",
    "\n",
    "plt.boxplot(y_val)\n",
    "plt.title(f'Boxplot for')\n",
    "plt.ylim(-0.1, 1)  # Set y-axis limit from 0 to 1\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.tight_layout()  # Ensure proper spacing\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.562, 0.046, 0.084, ..., 0.605, 0.574, 0.777], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
