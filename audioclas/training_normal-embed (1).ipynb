{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1123 14:42:16.001645 140034897893184 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating /srv/audio-classification-tf/models/2023-11-23_144216\n",
      "creating /srv/audio-classification-tf/models/2023-11-23_144216/logs\n",
      "creating /srv/audio-classification-tf/models/2023-11-23_144216/ckpts\n",
      "creating /srv/audio-classification-tf/models/2023-11-23_144216/conf\n",
      "creating /srv/audio-classification-tf/models/2023-11-23_144216/stats\n",
      "creating /srv/audio-classification-tf/models/2023-11-23_144216/plots\n",
      "creating /srv/audio-classification-tf/models/2023-11-23_144216/dataset_files\n",
      "creating /srv/audio-classification-tf/models/2023-11-23_144216/predictions\n",
      "Loading class names...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['GardenCity_15811', 'GardenCity_26981', 'Grafton_15810',\n",
       "       'Grafton_28434', 'Grafton_29187'], dtype='<U16')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training runfile\n",
    "\n",
    "Date: October 2019\n",
    "Author: Ignacio Heredia\n",
    "Email: iheredia@ifca.unican.es\n",
    "Github: ignacioheredia\n",
    "\n",
    "Description:\n",
    "This file contains the commands for training an audio classifier.\n",
    "\n",
    "Additional notes:\n",
    "* On the training routine: Preliminary tests show that using a custom lr multiplier for the lower layers yield to better\n",
    "results than freezing them at the beginning and unfreezing them after a few epochs like it is suggested in the Keras\n",
    "tutorials.\n",
    "\"\"\"\n",
    "\n",
    "#TODO List:\n",
    "\n",
    "#TODO: Implement resuming training\n",
    "#TODO: Try that everything works out with validation data\n",
    "#TODO: Try several regularization parameters\n",
    "#TODO: Add additional metrics for test time in addition to accuracy\n",
    "#TODO: Implement additional techniques to deal with class imbalance (not only class_weigths)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from audioclas import paths, config, utils, model_utils\n",
    "from audioclas.data_utils import load_data_splits, load_class_names, data_sequence, generate_embeddings,\\\n",
    "    file_to_PCM_16bits, json_friendly, save_embeddings_txt\n",
    "from audioclas.model import ModelWrapper\n",
    "from audioclas.optimizers import customAdam\n",
    "\n",
    "# Set Tensorflow verbosity logs\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Dynamically grow the memory used on the GPU (https://github.com/keras-team/keras/issues/4161)\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "tfconfig = tf.ConfigProto(gpu_options=gpu_options)\n",
    "sess = tf.Session(config=tfconfig)\n",
    "K.set_session(sess)\n",
    "\n",
    "\n",
    "CONF = config.get_conf_dict()\n",
    "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "\n",
    "CONF[\"general\"][\"dataset_directory\"]='/storage/Imagine_UC6/data_new_ais/data50'\n",
    "\n",
    "CONF[\"training\"][\"use_early_stopping\"]=True\n",
    "CONF[\"training\"][\"batch_size\"]=50\n",
    "CONF[\"training\"][\"epochs\"]=15\n",
    "\n",
    "CONF[\"preprocessing\"][\"files_to_PCM\"]=True\n",
    "CONF[\"preprocessing\"][\"compute_embeddings\"]=True\n",
    "\n",
    "paths.timestamp = TIMESTAMP\n",
    "paths.CONF = CONF\n",
    "\n",
    "utils.create_dir_tree()\n",
    "utils.backup_splits()\n",
    "load_class_names(splits_dir=paths.get_ts_splits_dir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF[\"training\"][\"batch_size\"]=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n",
      "new version\n",
      "Loading val data...\n",
      "new version\n",
      "Transforming inputs to PCM 16-bits ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [11:59<00:00,  5.56it/s]\n",
      "100%|██████████| 444/444 [01:24<00:00,  5.27it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing old embeddings ...\n",
      "Generating new embeddings ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "396it [02:20,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-02-23_03-24-54_306-0_2022-02-23-03-30-00_21_1590.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "454it [02:39,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-02-17_03-24-54_306-0_2022-02-17-03-30-00_21_2828.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "957it [05:27,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/Grafton_28434/2022-09-07_03-24-54_306-0_2022-09-07-03-30-00_21_2589.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1122it [06:22,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-02-27_03-24-54_306-0_2022-02-27-03-30-00_21_6853.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1231it [06:58,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-02-09_03-24-54_306-0_2022-02-09-03-30-00_21_1405.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1354it [07:38,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-02-15_03-24-54_306-0_2022-02-15-03-30-00_21_3738.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2014it [11:14,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-02-01_03-24-54_306-0_2022-02-01-03-30-00_21_4747.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2078it [11:35,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-03-01_03-24-54_306-0_2022-03-01-03-30-00_21_10312.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2286it [12:43,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/Grafton_28434/2022-08-20_03-24-54_306-0_2022-08-20-03-30-00_21_3590.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2787it [15:31,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-03-07_03-24-54_306-0_2022-03-07-03-30-00_21_1723.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2861it [15:55,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-02-07_03-24-54_306-0_2022-02-07-03-30-00_21_12607.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3333it [18:33,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-03-11_03-24-54_306-0_2022-03-11-03-30-00_21_1999.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3519it [19:33,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/Grafton_28434/2022-08-22_03-24-54_306-0_2022-08-22-03-30-00_21_1948.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3657it [20:18,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-05-18_03-24-54_306-0_2022-05-18-03-30-00_21_4832.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3803it [21:06,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-03-29_03-24-54_306-0_2022-03-29-03-30-00_21_1590.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4000it [22:12,  3.00it/s]\n",
      "167it [00:55,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-02-13_03-24-54_306-0_2022-02-13-03-30-00_21_5085.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "301it [01:39,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/GardenCity_15811/2022-02-19_03-24-54_306-0_2022-02-19-03-30-00_21_4553.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "433it [02:22,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipped /storage/Imagine_UC6/data_new_ais/data50/Grafton_28434/2022-08-24_03-24-54_306-0_2022-08-24-03-30-00_21_1944.wav because error in pre process embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "444it [02:25,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "X_train, y_train = load_data_splits(splits_dir=paths.get_ts_splits_dir(),\n",
    "                                    dataset_dir=paths.get_dataset_dir(),\n",
    "                                    split_name='train')\n",
    "\n",
    "# Load the validation data\n",
    "if (CONF['training']['use_validation']) and ('val.txt' in os.listdir(paths.get_ts_splits_dir())):\n",
    "    X_val, y_val = load_data_splits(splits_dir=paths.get_ts_splits_dir(),\n",
    "                                    dataset_dir=paths.get_dataset_dir(),\n",
    "                                    split_name='val')\n",
    "else:\n",
    "    print('No validation data.')\n",
    "    X_val, y_val = None, None\n",
    "    CONF['training']['use_validation'] = False\n",
    "\n",
    "# Load the class names\n",
    "class_names = 1 #load_class_names(splits_dir=paths.get_ts_splits_dir())\n",
    "\n",
    "# Update the configuration\n",
    "CONF['training']['batch_size'] = min(CONF['training']['batch_size'], len(X_train))\n",
    "\n",
    "if CONF['model']['num_classes'] is None:\n",
    "    CONF['model']['num_classes'] = 1#len(class_names)\n",
    "\n",
    "# assert CONF['model']['num_classes'] >= np.amax(y_train),\\\n",
    "#     \"Your train.txt file has more categories than those defined in classes.txt\"\n",
    "# if CONF['training']['use_validation']:\n",
    "#     assert CONF['model']['num_classes'] >= np.amax(y_val),\\\n",
    "#         \"Your val.txt file has more categories than those defined in classes.txt\"\n",
    "\n",
    "# Transform the training data to scipy-compatible 16-bit\n",
    "if not CONF['preprocessing']['compute_embeddings']:  # no need to compute if embeddings are precomputed\n",
    "    CONF['preprocessing']['files_to_PCM'] = False\n",
    "\n",
    "if CONF['preprocessing']['files_to_PCM']:\n",
    "    print('Transforming inputs to PCM 16-bits ...')\n",
    "    for p in tqdm(X_train):\n",
    "        file_to_PCM_16bits(p)\n",
    "    if CONF['training']['use_validation']:\n",
    "        for p in tqdm(X_val):\n",
    "            file_to_PCM_16bits(p)\n",
    "\n",
    "# Create model wrapper\n",
    "model_wrap = ModelWrapper(classifier_model=os.path.join(paths.get_models_dir(), 'audioset', 'ckpts',\n",
    "                                                        'final_model.h5'))\n",
    "\n",
    "# Generating new embeddings if needed\n",
    "if CONF['preprocessing']['compute_embeddings']:\n",
    "    print('Clearing old embeddings ...')\n",
    "    embed_dir = paths.get_embeddings_dir()\n",
    "#     for f in os.listdir(embed_dir):\n",
    "#         os.remove(os.path.join(embed_dir, f))\n",
    "\n",
    "    print(\"Generating new embeddings ...\")\n",
    "    X_train, y_train = generate_embeddings(model_wrap=model_wrap, filepaths=X_train, labels=y_train)\n",
    "    save_embeddings_txt(X_train, y_train, 'train_emb.txt')\n",
    "\n",
    "    if CONF['training']['use_validation']:\n",
    "        X_val, y_val = generate_embeddings(model_wrap=model_wrap, filepaths=X_val, labels=y_val)\n",
    "        save_embeddings_txt(X_val, y_val, 'val_emb.txt')\n",
    "\n",
    "else:\n",
    "    if not X_train[0].endswith('.npy'):\n",
    "        raise Exception(\"If you do not compute the embeddings, your train/val.txt should point to the\"\n",
    "                        \"embeddings `.npy` files.\")\n",
    "\n",
    "# Create data generator for train and val sets\n",
    "train_gen = data_sequence(X_train, y_train,\n",
    "                          batch_size=CONF['training']['batch_size'],\n",
    "                          num_classes=CONF['model']['num_classes'], shuffle=False)\n",
    "train_steps = int(np.ceil(len(X_train)/CONF['training']['batch_size']))\n",
    "\n",
    "if CONF['training']['use_validation']:\n",
    "    val_gen = data_sequence(X_val, y_val,\n",
    "                            batch_size=CONF['training']['batch_size'],\n",
    "                            num_classes=CONF['model']['num_classes'], shuffle=False)\n",
    "    val_steps = int(np.ceil(len(X_val)/CONF['training']['batch_size']))\n",
    "else:\n",
    "    val_gen = None\n",
    "    val_steps = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinePlotCallback(Callback):\n",
    "    def __init__(self, train_gen, val_gen, val_steps, class_names):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.val_steps = val_steps\n",
    "        self.class_names = class_names\n",
    "        self.epoch_count = 0\n",
    "        self.colors = get_cmap('tab20').colors\n",
    "        self.wout_avg_train = []\n",
    "        self.wout_avg_val = []\n",
    "        self.abs_diff_history_train = []\n",
    "        self.abs_diff_history_val = []\n",
    "        \n",
    "        \n",
    "        self.predicted_values_train=[]\n",
    "        self.predicted_values_val=[]\n",
    "        \n",
    "        self.real_values_val=[]\n",
    "        self.real_values_train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " def save_predicted_values( generator, predicted_values_list, real_values_list, abs_diff_history_list):\n",
    "        predicted_values_epoch = []\n",
    "        real_values_epoch = []\n",
    "\n",
    "        for batch in range(len(generator)):\n",
    "            batch_X, batch_y = generator[batch]\n",
    "            batch_y_pred = model.predict(batch_X)\n",
    "            predicted_values_epoch.append(batch_y_pred[0])\n",
    "            real_values_epoch.append(batch_y[0])\n",
    "\n",
    "            abs_diff = np.abs(batch_y - batch_y_pred)\n",
    "            avg_abs_diff = np.mean(abs_diff, axis=0)\n",
    "            abs_diff_history_list.append(avg_abs_diff)\n",
    "\n",
    "        predicted_values_list.append(predicted_values_epoch)\n",
    "        real_values_list.append(real_values_epoch)\n",
    "        return predicted_values_list, real_values_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_values_train = []\n",
    "abs_diff_history_train = []\n",
    "#         predicted_values_val = []\n",
    "#         real_values_val = []\n",
    "abs_diff_history_val = []\n",
    "self.predicted_values_train,self.real_values_train=save_predicted_values(self.train_gen, self.predicted_values_train, self.real_values_train, abs_diff_history_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abs_diff_history_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c0a0f22c9f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted_values_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_values_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_predicted_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted_values_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal_values_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_diff_history_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'abs_diff_history_train' is not defined"
     ]
    }
   ],
   "source": [
    "real_values_train = []\n",
    "abs_diff_history_train = []\n",
    "#         predicted_values_val = []\n",
    "#         real_values_val = []\n",
    "abs_diff_history_val = []\n",
    "\n",
    "self.predicted_values_train,self.real_values_train= self.save_predicted_values(self.train_gen, self.predicted_values_train, self.real_values_train, abs_diff_history_train)\n",
    "self.predicted_values_val,self.real_values_val=self.save_predicted_values(self.val_gen, self.predicted_values_val, self.real_values_val, abs_diff_history_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the abs_diff_history to get per-label data\n",
    "avg_abs_diff_history_train = np.array(abs_diff_history_train).T\n",
    "avg_abs_diff_history_val = np.array(abs_diff_history_val).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(avg_abs_diff_history_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wout_avg_train.append(np.mean(avg_abs_diff_history_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([0.18522519, 0.40682292, 0.36882296, 0.16368942, 0.41982293,\n",
       "       0.1931365 , 0.3648228 , 0.1659862 , 0.17093088, 0.18359202,\n",
       "       0.3856739 , 0.20724869, 0.21036863, 0.38291383, 0.22077331,\n",
       "       0.5041771 , 0.26108027, 0.16822617, 0.34703386, 0.43582287,\n",
       "       0.1931365 , 0.37279388, 0.22077331, 0.16007324, 0.19361645,\n",
       "       0.28898555, 0.19099957, 0.15868503, 0.15793313, 0.2689781 ,\n",
       "       0.16050571, 0.40217713, 0.43917713, 0.18534851, 0.42817706,\n",
       "       0.21088867, 0.20412865, 0.19363955, 0.24873239, 0.475177  ,\n",
       "       0.45082292, 0.1748655 , 0.46217716, 0.51117706, 0.4891771 ,\n",
       "       0.4458229 , 0.45282292, 0.2938229 , 0.5141771 , 0.3428229 ],\n",
       "      dtype=float32),\n",
       "       array([0.36546353, 0.3095365 , 0.14768143, 0.3117754 , 0.19406591,\n",
       "       0.25453648, 0.54146355, 0.11800705, 0.26453644, 0.12399098,\n",
       "       0.12232733, 0.20642643, 0.35953647, 0.14710173, 0.20090455,\n",
       "       0.13511027, 0.1299151 , 0.4165365 , 0.1391103 , 0.1347103 ,\n",
       "       0.37053642, 0.11577016, 0.15459566, 0.54146355, 0.35546345,\n",
       "       0.21099767, 0.11840706, 0.26153645, 0.17531987, 0.34053648,\n",
       "       0.2705364 , 0.12824479, 0.16181737, 0.2285065 , 0.22178654,\n",
       "       0.11900707, 0.3824636 , 0.27253646, 0.40953645, 0.13288963,\n",
       "       0.1156607 , 0.4984635 , 0.15388459, 0.15907568, 0.1274789 ,\n",
       "       0.1355103 , 0.1238789 , 0.23914577, 0.2425058 , 0.30353644],\n",
       "      dtype=float32),\n",
       "       array([0.1728786 , 0.15719734, 0.2390609 , 0.18551436, 0.15655741,\n",
       "       0.41008157, 0.15669279, 0.45091847, 0.16367453, 0.33008155,\n",
       "       0.16572489, 0.27102712, 0.16740485, 0.39591843, 0.16443531,\n",
       "       0.4979185 , 0.15846866, 0.32708153, 0.46408153, 0.17545693,\n",
       "       0.5029185 , 0.43208158, 0.1829544 , 0.39191845, 0.48308152,\n",
       "       0.34017688, 0.3450815 , 0.30258784, 0.3870815 , 0.21434547,\n",
       "       0.37691852, 0.15942869, 0.5029185 , 0.45291847, 0.16415529,\n",
       "       0.15482225, 0.16740485, 0.27254713, 0.41791844, 0.4559185 ,\n",
       "       0.26015455, 0.27666593, 0.16516486, 0.19143213, 0.15751739,\n",
       "       0.45391855, 0.18679437, 0.27796218, 0.34608155, 0.16908485],\n",
       "      dtype=float32),\n",
       "       array([0.17392063, 0.3241764 , 0.15122163, 0.14734288, 0.14490813,\n",
       "       0.1790266 , 0.15421872, 0.27578914, 0.15203866, 0.20279239,\n",
       "       0.5698236 , 0.5698236 , 0.18860045, 0.29617643, 0.31917644,\n",
       "       0.34517643, 0.14525414, 0.29617643, 0.18692046, 0.14784501,\n",
       "       0.18577985, 0.26786032, 0.14496498, 0.5078236 , 0.40317643,\n",
       "       0.17950659, 0.5698236 , 0.21601985, 0.51782364, 0.31317642,\n",
       "       0.35417643, 0.5698236 , 0.4338235 , 0.1454329 , 0.17901473,\n",
       "       0.41182363, 0.15577643, 0.17480062, 0.37517634, 0.34517643,\n",
       "       0.23282781, 0.2075924 , 0.15741378, 0.3101764 , 0.16112272,\n",
       "       0.16757359, 0.144925  , 0.16048273, 0.27854913, 0.19199234],\n",
       "      dtype=float32),\n",
       "       array([0.15572682, 0.29665768, 0.17167386, 0.45006004, 0.21776132,\n",
       "       0.20343868, 0.27645007, 0.35205996, 0.1947987 , 0.30108577,\n",
       "       0.44105998, 0.49694008, 0.1897713 , 0.43494007, 0.15625793,\n",
       "       0.32894003, 0.20535871, 0.24812764, 0.49694008, 0.17846386,\n",
       "       0.42305997, 0.20439877, 0.16480528, 0.15600541, 0.1557268 ,\n",
       "       0.24924347, 0.4590599 , 0.49694008, 0.4160599 , 0.23703891,\n",
       "       0.40194008, 0.17251389, 0.26252756, 0.2678446 , 0.49694008,\n",
       "       0.49694008, 0.35205996, 0.15572682, 0.41905993, 0.43705997,\n",
       "       0.43705997, 0.22301365, 0.18966918, 0.22237368, 0.2032913 ,\n",
       "       0.4070599 , 0.33705983, 0.41294006, 0.15572682, 0.46494004],\n",
       "      dtype=float32),\n",
       "       array([0.18395896, 0.4112964 , 0.22518492, 0.20923343, 0.3282963 ,\n",
       "       0.15271026, 0.5237037 , 0.5237037 , 0.44870365, 0.45570362,\n",
       "       0.1639477 , 0.39229637, 0.41729626, 0.24357249, 0.18251914,\n",
       "       0.13441187, 0.14242506, 0.13782138, 0.27980325, 0.19691914,\n",
       "       0.5237037 , 0.13407426, 0.17771913, 0.40229636, 0.13407427,\n",
       "       0.16634773, 0.16528688, 0.15809429, 0.33770362, 0.18827897,\n",
       "       0.5237037 , 0.13846412, 0.24261934, 0.13990417, 0.5237037 ,\n",
       "       0.1347318 , 0.19611917, 0.3032964 , 0.14042504, 0.14184603,\n",
       "       0.18011919, 0.50670373, 0.4957036 , 0.37129635, 0.5237037 ,\n",
       "       0.15299715, 0.45170364, 0.13589694, 0.14362504, 0.206705  ],\n",
       "      dtype=float32),\n",
       "       array([0.31143552, 0.15277073, 0.1546791 , 0.23169143, 0.1568975 ,\n",
       "       0.1737665 , 0.3695644 , 0.2887395 , 0.15402639, 0.2624579 ,\n",
       "       0.43256435, 0.17037039, 0.21813579, 0.4354356 , 0.15249074,\n",
       "       0.19040836, 0.51443565, 0.2624579 , 0.1849829 , 0.16884716,\n",
       "       0.38656443, 0.19606201, 0.24595371, 0.18452202, 0.18266581,\n",
       "       0.28075993, 0.23529148, 0.28793952, 0.16965038, 0.4145644 ,\n",
       "       0.3027524 , 0.3074356 , 0.46343562, 0.38443556, 0.37756443,\n",
       "       0.28393948, 0.17829041, 0.2716979 , 0.35243556, 0.24249151,\n",
       "       0.44556445, 0.19264834, 0.46156445, 0.39056438, 0.39856443,\n",
       "       0.27695993, 0.51443565, 0.4205644 , 0.17280647, 0.30611238],\n",
       "      dtype=float32),\n",
       "       array([0.14667577, 0.22702038, 0.2453022 , 0.4857435 , 0.14519572,\n",
       "       0.3992566 , 0.151267  , 0.28901052, 0.28421053, 0.19505978,\n",
       "       0.21998043, 0.2140165 , 0.23426226, 0.13793968, 0.21317646,\n",
       "       0.16650656, 0.18476385, 0.171623  , 0.27290225, 0.16143942,\n",
       "       0.21222252, 0.3497434 , 0.42025653, 0.5327434 , 0.17217158,\n",
       "       0.16143942, 0.14178327, 0.30053058, 0.13720864, 0.2775588 ,\n",
       "       0.13704862, 0.29957056, 0.3432566 , 0.14235018, 0.19625363,\n",
       "       0.18362297, 0.14051239, 0.14827575, 0.3497434 , 0.41625655,\n",
       "       0.14051239, 0.40074345, 0.19945358, 0.43225658, 0.36925653,\n",
       "       0.3322566 , 0.13995019, 0.13704765, 0.13923241, 0.27566218],\n",
       "      dtype=float32),\n",
       "       array([0.37402314, 0.37297684, 0.3619769 , 0.17240068, 0.21714644,\n",
       "       0.16060227, 0.16564226, 0.52102315, 0.171994  , 0.23755613,\n",
       "       0.3155682 , 0.17988066, 0.1820936 , 0.23033017, 0.15123843,\n",
       "       0.16240223, 0.39202318, 0.15354525, 0.17667402, 0.19005413,\n",
       "       0.14928104, 0.16373219, 0.52102315, 0.43997687, 0.19339645,\n",
       "       0.34532824, 0.40697682, 0.26104262, 0.14976104, 0.25273013,\n",
       "       0.20967829, 0.52102315, 0.16801038, 0.41397685, 0.51702315,\n",
       "       0.44002306, 0.14949016, 0.23913014, 0.41997683, 0.2102783 ,\n",
       "       0.15844226, 0.2569853 , 0.17969358, 0.40497676, 0.45297688,\n",
       "       0.149721  , 0.24713011, 0.15203844, 0.40697682, 0.16024227],\n",
       "      dtype=float32),\n",
       "       array([0.1899374 , 0.20707864, 0.2933336 , 0.21067862, 0.16198759,\n",
       "       0.4842134 , 0.3393004 , 0.4107865 , 0.42578667, 0.18721737,\n",
       "       0.19537733, 0.15827559, 0.15547897, 0.15636088, 0.4497866 ,\n",
       "       0.36578655, 0.25019807, 0.15627894, 0.20777048, 0.3967865 ,\n",
       "       0.3222134 , 0.41821343, 0.2742037 , 0.16134758, 0.22197671,\n",
       "       0.38821343, 0.1600676 , 0.3867866 , 0.4557866 , 0.17117012,\n",
       "       0.3448204 , 0.17221016, 0.15564086, 0.31722048, 0.33321342,\n",
       "       0.17548569, 0.3337804 , 0.15074795, 0.22503456, 0.19333737,\n",
       "       0.15950695, 0.21578594, 0.15050797, 0.39021343, 0.16114406,\n",
       "       0.15707898, 0.3015804 , 0.15447898, 0.15444085, 0.27572367],\n",
       "      dtype=float32),\n",
       "       array([0.14532924, 0.20741135, 0.16325621, 0.22942439, 0.1556115 ,\n",
       "       0.3849007 , 0.14949413, 0.17557581, 0.35990074, 0.14699998,\n",
       "       0.1800558 , 0.17085496, 0.4840992 , 0.14797585, 0.20597132,\n",
       "       0.21045138, 0.2980142 , 0.1648162 , 0.2962542 , 0.48190075,\n",
       "       0.14716   , 0.14708   , 0.16973495, 0.50890076, 0.33465618,\n",
       "       0.47690076, 0.2435977 , 0.21125042, 0.45190066, 0.37390077,\n",
       "       0.38809928, 0.15225722, 0.39909923, 0.17813586, 0.1642962 ,\n",
       "       0.15497717, 0.17314586, 0.2180552 , 0.4840992 , 0.4840992 ,\n",
       "       0.36509925, 0.3789007 , 0.45990083, 0.25525957, 0.17941584,\n",
       "       0.32809925, 0.39009926, 0.19030735, 0.18601729, 0.14569676],\n",
       "      dtype=float32),\n",
       "       array([0.18173416, 0.16626337, 0.22530955, 0.166261  , 0.15786372,\n",
       "       0.24622995, 0.2860011 , 0.3451692 , 0.15892215, 0.50446206,\n",
       "       0.42946202, 0.40053794, 0.22746956, 0.23539904, 0.1750198 ,\n",
       "       0.28264102, 0.35668916, 0.50446206, 0.17958227, 0.16275461,\n",
       "       0.21155812, 0.15766364, 0.18660533, 0.1791823 , 0.21923815,\n",
       "       0.32553798, 0.27172104, 0.34132913, 0.17533982, 0.50446206,\n",
       "       0.48946202, 0.15766364, 0.30196106, 0.42053807, 0.50446206,\n",
       "       0.17373978, 0.25367263, 0.19060534, 0.50446206, 0.20899813,\n",
       "       0.15766367, 0.1782198 , 0.16275461, 0.40346202, 0.1740598 ,\n",
       "       0.36853805, 0.38653794, 0.16052133, 0.34753796, 0.24078292],\n",
       "      dtype=float32),\n",
       "       array([0.15410106, 0.25504482, 0.26952413, 0.22258638, 0.21718638,\n",
       "       0.18004534, 0.1724586 , 0.3313541 , 0.23598847, 0.22318846,\n",
       "       0.3457466 , 0.1570293 , 0.16652316, 0.39304143, 0.41004145,\n",
       "       0.15199104, 0.15822929, 0.17452313, 0.22830847, 0.35534653,\n",
       "       0.15127105, 0.14966625, 0.34295857, 0.39304143, 0.4730414 ,\n",
       "       0.29896417, 0.14990625, 0.34094658, 0.45804155, 0.25449   ,\n",
       "       0.14928423, 0.43695858, 0.5019586 , 0.41904148, 0.4089586 ,\n",
       "       0.5019586 , 0.31552413, 0.15247104, 0.20415674, 0.30363122,\n",
       "       0.23342846, 0.4810414 , 0.18318455, 0.22446838, 0.27596414,\n",
       "       0.20945397, 0.45604146, 0.44704148, 0.18836531, 0.46895847],\n",
       "      dtype=float32),\n",
       "       array([0.13753588, 0.2699312 , 0.5486276 , 0.16097444, 0.3448662 ,\n",
       "       0.5486276 , 0.20036158, 0.13825016, 0.14690785, 0.13258585,\n",
       "       0.13456696, 0.18891138, 0.2854368 , 0.19280984, 0.3333462 ,\n",
       "       0.5486276 , 0.18495819, 0.37762764, 0.3381462 , 0.4103725 ,\n",
       "       0.18556614, 0.13445014, 0.3477463 , 0.21260154, 0.29556745,\n",
       "       0.17419782, 0.22558084, 0.15080877, 0.23884413, 0.27899686,\n",
       "       0.28359684, 0.35412747, 0.13258585, 0.46562752, 0.15194896,\n",
       "       0.23644413, 0.5486276 , 0.14346668, 0.5486276 , 0.5076274 ,\n",
       "       0.15378933, 0.4103725 , 0.38737246, 0.1328674 , 0.28884748,\n",
       "       0.42237243, 0.15514925, 0.33526623, 0.24284409, 0.3544663 ],\n",
       "      dtype=float32),\n",
       "       array([0.4129385 , 0.1542638 , 0.14526601, 0.1619922 , 0.18049572,\n",
       "       0.24381928, 0.38193855, 0.14497875, 0.16343433, 0.16786593,\n",
       "       0.28879726, 0.16375434, 0.44606155, 0.14751892, 0.15114649,\n",
       "       0.15245867, 0.29263732, 0.15927432, 0.17713566, 0.1588611 ,\n",
       "       0.21349911, 0.21189915, 0.1542638 , 0.2878373 , 0.15128848,\n",
       "       0.17967416, 0.2263914 , 0.2511792 , 0.42193845, 0.2199114 ,\n",
       "       0.19444536, 0.21429913, 0.3521573 , 0.4269386 , 0.39793846,\n",
       "       0.27034456, 0.14522603, 0.42106143, 0.16038631, 0.14539962,\n",
       "       0.20232925, 0.15746112, 0.20458513, 0.14518602, 0.16523223,\n",
       "       0.3579386 , 0.15165862, 0.15258156, 0.1463596 , 0.15145868],\n",
       "      dtype=float32),\n",
       "       array([0.28812686, 0.18061681, 0.16572481, 0.4059112 , 0.16842413,\n",
       "       0.27465552, 0.43591127, 0.24084178, 0.50408876, 0.16564493,\n",
       "       0.31452686, 0.18181202, 0.4329112 , 0.4650888 , 0.16608478,\n",
       "       0.20450556, 0.16882363, 0.38491124, 0.27015048, 0.4689112 ,\n",
       "       0.5050887 , 0.23482236, 0.1670641 , 0.46808872, 0.23022236,\n",
       "       0.45191112, 0.4760888 , 0.27495044, 0.16564494, 0.36591125,\n",
       "       0.16698413, 0.17164211, 0.18974072, 0.1849274 , 0.4689112 ,\n",
       "       0.18452749, 0.17964906, 0.4250888 , 0.35591123, 0.2019036 ,\n",
       "       0.37591127, 0.42408878, 0.34884682, 0.17532898, 0.38753676,\n",
       "       0.16564494, 0.1934371 , 0.1661041 , 0.27625555, 0.17377765],\n",
       "      dtype=float32),\n",
       "       array([0.16351652, 0.42543304, 0.16711651, 0.20726383, 0.17053422,\n",
       "       0.42656693, 0.46156684, 0.14977112, 0.14853658, 0.17503652,\n",
       "       0.16351652, 0.3675669 , 0.14929113, 0.15824288, 0.44656682,\n",
       "       0.14776035, 0.41843304, 0.24175352, 0.36356685, 0.43256682,\n",
       "       0.3744331 , 0.16927652, 0.36143312, 0.15085833, 0.16113764,\n",
       "       0.36656684, 0.15125689, 0.14881058, 0.42956692, 0.14776033,\n",
       "       0.19158302, 0.14929113, 0.29442394, 0.15909855, 0.14821658,\n",
       "       0.5184331 , 0.15623257, 0.19314303, 0.1649565 , 0.41056693,\n",
       "       0.2625535 , 0.24015598, 0.14989112, 0.15549833, 0.1663965 ,\n",
       "       0.23111492, 0.21076132, 0.2713776 , 0.16855654, 0.4145668 ],\n",
       "      dtype=float32),\n",
       "       array([0.1520319 , 0.2992702 , 0.23081732, 0.32513902, 0.23297738,\n",
       "       0.15123083, 0.25924388, 0.24614376, 0.18841061, 0.15699573,\n",
       "       0.48513904, 0.41713905, 0.15935938, 0.441861  , 0.16134366,\n",
       "       0.361139  , 0.44486105, 0.2790302 , 0.2192974 , 0.24690382,\n",
       "       0.15507573, 0.15461794, 0.15114851, 0.3139903 , 0.400139  ,\n",
       "       0.15635571, 0.24034508, 0.48513904, 0.16075937, 0.48513904,\n",
       "       0.332139  , 0.27736396, 0.45613897, 0.453861  , 0.2753503 ,\n",
       "       0.22649729, 0.15182728, 0.26457918, 0.15107045, 0.1515873 ,\n",
       "       0.344861  , 0.48513904, 0.48486102, 0.3358609 , 0.17433253,\n",
       "       0.15134852, 0.47986102, 0.40586093, 0.344861  , 0.42586106],\n",
       "      dtype=float32),\n",
       "       array([0.1602438 , 0.17498332, 0.47511995, 0.27811995, 0.20836696,\n",
       "       0.18252902, 0.35979939, 0.18120904, 0.19274451, 0.21005908,\n",
       "       0.15572375, 0.31960475, 0.34911996, 0.17534333, 0.1772044 ,\n",
       "       0.3481199 , 0.35711992, 0.4818802 , 0.15345842, 0.47511995,\n",
       "       0.47511995, 0.17714329, 0.32604474, 0.4718802 , 0.23260286,\n",
       "       0.19436443, 0.24139023, 0.21843284, 0.3651199 , 0.15227221,\n",
       "       0.23336281, 0.3398447 , 0.32696468, 0.47511995, 0.1566264 ,\n",
       "       0.18296903, 0.19392444, 0.18650451, 0.3540393 , 0.16850334,\n",
       "       0.16770418, 0.2920047 , 0.20286745, 0.32052472, 0.16108377,\n",
       "       0.15137841, 0.41188008, 0.32211995, 0.20346744, 0.45711997],\n",
       "      dtype=float32),\n",
       "       array([0.20624179, 0.21879749, 0.3213445 , 0.18426165, 0.18309015,\n",
       "       0.4521848 , 0.34470287, 0.43118477, 0.26966327, 0.15838258,\n",
       "       0.45881528, 0.16974379, 0.42181522, 0.1607246 , 0.35005206,\n",
       "       0.44918478, 0.17338255, 0.16974379, 0.2880921 , 0.4248152 ,\n",
       "       0.21955748, 0.4288152 , 0.24669212, 0.20043209, 0.17471983,\n",
       "       0.21688175, 0.43481526, 0.32213214, 0.16144465, 0.4481848 ,\n",
       "       0.18257019, 0.19200134, 0.15808459, 0.16060457, 0.17508766,\n",
       "       0.3429908 , 0.37381527, 0.16240466, 0.32386443, 0.27429217,\n",
       "       0.5271847 , 0.17564768, 0.1578156 , 0.19071826, 0.3543029 ,\n",
       "       0.24485214, 0.21913996, 0.20512174, 0.15937616, 0.23874825],\n",
       "      dtype=float32),\n",
       "       array([0.151761  , 0.5059564 , 0.19438973, 0.150801  , 0.17458317,\n",
       "       0.48995632, 0.19814873, 0.29176176, 0.42204365, 0.4760437 ,\n",
       "       0.15833321, 0.18514314, 0.16179352, 0.15475756, 0.39704368,\n",
       "       0.46504375, 0.17746313, 0.17506315, 0.3429563 , 0.34895638,\n",
       "       0.23554035, 0.34804368, 0.15053236, 0.166129  , 0.4109564 ,\n",
       "       0.15635756, 0.15897323, 0.15367965, 0.31864178, 0.3770437 ,\n",
       "       0.15001236, 0.14997235, 0.18932581, 0.22794035, 0.1911194 ,\n",
       "       0.18658316, 0.15041237, 0.15747756, 0.17122312, 0.17266314,\n",
       "       0.41204357, 0.15017235, 0.30808178, 0.23250027, 0.151201  ,\n",
       "       0.15891758, 0.22187267, 0.16147943, 0.16337934, 0.15387967],\n",
       "      dtype=float32),\n",
       "       array([0.16615021, 0.3105371 , 0.45832503, 0.17500088, 0.18076088,\n",
       "       0.30195123, 0.16631018, 0.485675  , 0.33967495, 0.20050664,\n",
       "       0.16607398, 0.17473291, 0.41067493, 0.36272386, 0.20050664,\n",
       "       0.44732514, 0.41832507, 0.47832504, 0.18408544, 0.17839473,\n",
       "       0.3512038 , 0.3105371 , 0.30173716, 0.5186749 , 0.1718809 ,\n",
       "       0.29381713, 0.192978  , 0.19327864, 0.5186749 , 0.17166676,\n",
       "       0.38232517, 0.1754529 , 0.41467488, 0.16812415, 0.18004084,\n",
       "       0.19101435, 0.20506762, 0.2709371 , 0.18806885, 0.3756749 ,\n",
       "       0.23201057, 0.40732506, 0.16908075, 0.3983251 , 0.4096749 ,\n",
       "       0.16766618, 0.17692085, 0.20558758, 0.33172753, 0.2406506 ],\n",
       "      dtype=float32),\n",
       "       array([0.19913687, 0.17406552, 0.18216342, 0.19817692, 0.18829809,\n",
       "       0.23405483, 0.16185221, 0.16129223, 0.42074326, 0.51425695,\n",
       "       0.20745915, 0.15734987, 0.51425695, 0.51425695, 0.4312568 ,\n",
       "       0.2870061 , 0.35925686, 0.15573177, 0.25205848, 0.39874318,\n",
       "       0.16241223, 0.2347348 , 0.2566396 , 0.41174325, 0.35725677,\n",
       "       0.36625683, 0.2641204 , 0.15652311, 0.22660273, 0.3333311 ,\n",
       "       0.2381348 , 0.2004034 , 0.3842568 , 0.15497178, 0.20521909,\n",
       "       0.18408342, 0.32045114, 0.35725114, 0.1679455 , 0.16210733,\n",
       "       0.2945661 , 0.16101223, 0.22532271, 0.37105113, 0.44974324,\n",
       "       0.32873112, 0.4162568 , 0.16213222, 0.23677485, 0.16070734],\n",
       "      dtype=float32),\n",
       "       array([0.4714161 , 0.42241615, 0.3007025 , 0.22939707, 0.49258366,\n",
       "       0.42758378, 0.35841632, 0.49258366, 0.45958385, 0.3534161 ,\n",
       "       0.16985945, 0.21022853, 0.3744161 , 0.16756432, 0.1373688 ,\n",
       "       0.21419711, 0.30266798, 0.25164402, 0.41941613, 0.16716431,\n",
       "       0.13582024, 0.4255838 , 0.16325943, 0.17045942, 0.16985945,\n",
       "       0.3985837 , 0.49258366, 0.18103212, 0.15133128, 0.17697455,\n",
       "       0.1973085 , 0.2429971 , 0.35958382, 0.20718847, 0.26900664,\n",
       "       0.17173626, 0.35641617, 0.14895014, 0.24176401, 0.32186258,\n",
       "       0.2807666 , 0.3222328 , 0.13020793, 0.21259715, 0.28544456,\n",
       "       0.21819712, 0.13917561, 0.301788  , 0.18204656, 0.25819713],\n",
       "      dtype=float32),\n",
       "       array([0.13314016, 0.43096316, 0.13481165, 0.33830994, 0.43496308,\n",
       "       0.12548946, 0.13589166, 0.28857446, 0.39896312, 0.20263882,\n",
       "       0.19471882, 0.17292573, 0.13157661, 0.20352384, 0.23540197,\n",
       "       0.22495884, 0.3510369 , 0.16898681, 0.12378199, 0.31893447,\n",
       "       0.2237733 , 0.30703694, 0.16578682, 0.32403696, 0.34310985,\n",
       "       0.25214854, 0.28673443, 0.26833448, 0.1283045 , 0.20911877,\n",
       "       0.2019188 , 0.2321588 , 0.28903696, 0.16125052, 0.21487884,\n",
       "       0.12596276, 0.12596728, 0.16322681, 0.34403694, 0.29403692,\n",
       "       0.20911877, 0.49803698, 0.42096305, 0.15789051, 0.36326993,\n",
       "       0.15159425, 0.45696297, 0.25626352, 0.1504336 , 0.1645068 ],\n",
       "      dtype=float32),\n",
       "       array([0.1584534 , 0.14699703, 0.33505666, 0.17376769, 0.14531706,\n",
       "       0.44494343, 0.38005653, 0.14480175, 0.3150565 , 0.213215  ,\n",
       "       0.3680566 , 0.33485273, 0.54294336, 0.54294336, 0.23426299,\n",
       "       0.29905656, 0.14320113, 0.15248476, 0.14062124, 0.16205339,\n",
       "       0.17747533, 0.21672276, 0.1712534 , 0.14630067, 0.26854447,\n",
       "       0.3599435 , 0.34905663, 0.48394337, 0.37105647, 0.30688164,\n",
       "       0.36194348, 0.15502338, 0.16765343, 0.14141327, 0.19553496,\n",
       "       0.14420113, 0.54294336, 0.25523353, 0.14320113, 0.19893502,\n",
       "       0.20432945, 0.2190028 , 0.14783704, 0.20165496, 0.16325343,\n",
       "       0.24323346, 0.14951706, 0.14004365, 0.14559706, 0.2778015 ],\n",
       "      dtype=float32),\n",
       "       array([0.15551555, 0.24764676, 0.14821562, 0.16360812, 0.18236642,\n",
       "       0.16897571, 0.15045273, 0.23552622, 0.16191326, 0.37873515,\n",
       "       0.1455553 , 0.17444009, 0.3180407 , 0.14651528, 0.44826478,\n",
       "       0.25448382, 0.38473535, 0.39273512, 0.27296376, 0.32956073,\n",
       "       0.4287352 , 0.27128232, 0.1640081 , 0.24242122, 0.51626474,\n",
       "       0.1432496 , 0.44373524, 0.39873523, 0.51626474, 0.31228068,\n",
       "       0.15461023, 0.14293458, 0.17204872, 0.18978019, 0.30572316,\n",
       "       0.16680811, 0.14453769, 0.1485821 , 0.19899319, 0.15687157,\n",
       "       0.51626474, 0.51626474, 0.31277025, 0.15455553, 0.4277353 ,\n",
       "       0.51626474, 0.14567529, 0.14958209, 0.17444874, 0.51626474],\n",
       "      dtype=float32),\n",
       "       array([0.39544895, 0.16375145, 0.24235079, 0.31858256, 0.21374656,\n",
       "       0.21688463, 0.27919346, 0.52244896, 0.35255107, 0.250359  ,\n",
       "       0.13028038, 0.2113418 , 0.13605028, 0.52244896, 0.2889991 ,\n",
       "       0.2077418 , 0.12974861, 0.46644905, 0.14377461, 0.32818267,\n",
       "       0.1300404 , 0.13028038, 0.1300404 , 0.52244896, 0.52244896,\n",
       "       0.1490987 , 0.18083291, 0.21100466, 0.1575499 , 0.1488344 ,\n",
       "       0.413449  , 0.14399366, 0.17471291, 0.13465029, 0.19247238,\n",
       "       0.34955102, 0.1321066 , 0.13070132, 0.42455104, 0.24587078,\n",
       "       0.42144895, 0.2861337 , 0.45655105, 0.16482025, 0.30802262,\n",
       "       0.21772459, 0.13046134, 0.13102901, 0.15706636, 0.25955907],\n",
       "      dtype=float32),\n",
       "       array([0.51620907, 0.24907637, 0.14841382, 0.3151229 , 0.2895872 ,\n",
       "       0.24507636, 0.50320905, 0.51620907, 0.1468012 , 0.15115085,\n",
       "       0.17437917, 0.41620913, 0.32839492, 0.44620904, 0.296715  ,\n",
       "       0.4457909 , 0.37679097, 0.14629655, 0.31416303, 0.15698576,\n",
       "       0.20953384, 0.14976113, 0.14971085, 0.41779092, 0.442791  ,\n",
       "       0.3239949 , 0.19675322, 0.18115321, 0.22848673, 0.15447703,\n",
       "       0.3327949 , 0.21430045, 0.15351702, 0.22312166, 0.2854834 ,\n",
       "       0.33103496, 0.1769769 , 0.1749132 , 0.40620902, 0.16144928,\n",
       "       0.17005916, 0.33015487, 0.1473907 , 0.1473907 , 0.29407495,\n",
       "       0.421791  , 0.51620907, 0.26213288, 0.15043087, 0.16381916],\n",
       "      dtype=float32),\n",
       "       array([0.44354913, 0.39945093, 0.148581  , 0.14567208, 0.15743884,\n",
       "       0.16718607, 0.4695491 , 0.28890756, 0.16703442, 0.15078099,\n",
       "       0.19229412, 0.15445434, 0.3975491 , 0.28890756, 0.14425096,\n",
       "       0.20744705, 0.45654908, 0.14551654, 0.34545094, 0.16963442,\n",
       "       0.1603909 , 0.17410667, 0.27457428, 0.40254906, 0.15305436,\n",
       "       0.23751375, 0.15885435, 0.1765633 , 0.14438431, 0.19022746,\n",
       "       0.21329437, 0.5435491 , 0.14660548, 0.14495005, 0.15745436,\n",
       "       0.28890756, 0.24098046, 0.556549  , 0.34345087, 0.17554106,\n",
       "       0.42545098, 0.2242944 , 0.14985436, 0.14458935, 0.22375384],\n",
       "      dtype=float32)], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(avg_abs_diff_history_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "self=LinePlotCallback(train_gen, val_gen, val_steps, class_names)\n",
    "# a.on_epoch_end(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from matplotlib.cm import get_cmap\n",
    "class LinePlotCallback(Callback):\n",
    "    def __init__(self, train_gen, val_gen, val_steps, class_names):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.val_steps = val_steps\n",
    "        self.class_names = class_names\n",
    "        self.epoch_count = 0\n",
    "        self.colors = get_cmap('tab20').colors\n",
    "        self.wout_avg_train = []\n",
    "        self.wout_avg_val = []\n",
    "        self.abs_diff_history_train = []\n",
    "        self.abs_diff_history_val = []\n",
    "        \n",
    "        \n",
    "        self.predicted_values_train=[]\n",
    "        self.predicted_values_val=[]\n",
    "        \n",
    "        self.real_values_val=[]\n",
    "        self.real_values_train=[]\n",
    "\n",
    "    def save_predicted_values(self, generator, predicted_values_list, real_values_list, abs_diff_history_list,datapoints):\n",
    "        predicted_values_epoch = []\n",
    "        real_values_epoch = []\n",
    "\n",
    "        for batch in range(len(generator)):\n",
    "            batch_X, batch_y = generator[batch]\n",
    "            batch_y_pred = self.model.predict(batch_X)\n",
    "            predicted_values_epoch.append(batch_y_pred[0:datapoints])\n",
    "            real_values_epoch.append(batch_y[0:datapoints])\n",
    "\n",
    "            abs_diff = np.abs(batch_y - batch_y_pred)\n",
    "            avg_abs_diff = np.mean(abs_diff)#, axis=0)\n",
    "            abs_diff_history_list.append(avg_abs_diff)\n",
    "\n",
    "        predicted_values_list.append(predicted_values_epoch)\n",
    "        real_values_list.append(real_values_epoch)\n",
    "        return predicted_values_list, real_values_list\n",
    "\n",
    "    def transform_values(self,values):\n",
    "            return [(10000 - 10000 * y) for y in values]\n",
    "        \n",
    "    def plot_batch_predicted_and_real(self, batch, predicted_values_list, real_values_list, save_dir, title, datapoints, is_validation=False):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        # Plot real values only once for each batch\n",
    "        real_values = real_values_list[0][batch]\n",
    "        names=[]\n",
    "        for i in range(datapoints):\n",
    "            names.append(f\"dp {i}\")\n",
    "\n",
    "        real_values = self.transform_values(real_values)\n",
    "        predicted_values_list = [self.transform_values(epoch_values) for epoch_values in predicted_values_list]\n",
    "\n",
    "\n",
    "        plt.scatter(names, real_values, label='Real', alpha=0.7, color='black', marker='o', s=100)\n",
    "\n",
    "        num_epochs = len(predicted_values_list)\n",
    "\n",
    "        for epoch, predicted_values_epoch in enumerate(predicted_values_list):\n",
    "            predicted_values = predicted_values_epoch[batch]\n",
    "            color = self.colors[epoch % len(self.colors)]\n",
    "#             marker = self.markers[epoch % len(self.markers)]\n",
    "            alpha = 1-(1.0 - (epoch / (num_epochs)) )\n",
    "            alpha= (epoch + 1) / num_epochs \n",
    "            plt.scatter(names, predicted_values, label=f'Epoch {epoch + 1}, Predicted', alpha=alpha, color=color)\n",
    "\n",
    "        plt.ylim(0, 10000)\n",
    "        plt.xlabel('Class Names')\n",
    "        plt.ylabel('Values')\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "\n",
    "        if is_validation:\n",
    "            plt.savefig(os.path.join(save_dir, f'validation_batch_{batch}.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(save_dir, f'training_batch_{batch}.png'))\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def plot_values_for_iterations(self, wout_avg, abs_diff_history, save_dir, title, is_validation=False):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        num_iterations = len(wout_avg)\n",
    "        num_classes = len(self.class_names)\n",
    "\n",
    "        # Create a DataFrame from the list\n",
    "        iterations = [f'Iteration {i + 1}' for i in range(num_iterations)]\n",
    "\n",
    "        for i in range(len(self.class_names)):\n",
    "            plt.plot(range(num_iterations), [wout_avg[j][i] for j in range(num_iterations)], label=self.class_names[i])\n",
    "\n",
    "        # Set x-tick labels\n",
    "        plt.xticks(range(num_iterations), iterations)\n",
    "\n",
    "        # Add a legend\n",
    "        plt.legend()\n",
    "\n",
    "        # Set labels and title\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Values')\n",
    "        plt.title(title)\n",
    "\n",
    "        if is_validation:\n",
    "            plt.savefig(os.path.join(save_dir, f'{title}_validation.png'))\n",
    "        else:\n",
    "            plt.savefig(os.path.join(save_dir, f'{title}_training.png'))\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_count = epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Save predicted values for training and validation\n",
    "#         predicted_values_train = []\n",
    "#         real_values_train = []\n",
    "        abs_diff_history_train = []\n",
    "#         predicted_values_val = []\n",
    "#         real_values_val = []\n",
    "        abs_diff_history_val = []\n",
    "        datapoints=10\n",
    "        self.predicted_values_train,self.real_values_train= self.save_predicted_values(self.train_gen, self.predicted_values_train, self.real_values_train, abs_diff_history_train,datapoints)\n",
    "        self.predicted_values_val,self.real_values_val=self.save_predicted_values(self.val_gen, self.predicted_values_val, self.real_values_val, abs_diff_history_val,datapoints)\n",
    "\n",
    "        # Plot batch predicted values and real values for training and validation\n",
    "        save_dir = paths.get_plots_dir()  # Replace with your actual directory path\n",
    "\n",
    "        for batch in range(len(self.predicted_values_train[0])):\n",
    "            self.plot_batch_predicted_and_real(\n",
    "                batch, self.predicted_values_train, self.real_values_train,\n",
    "                save_dir, f'Batch {batch} Predicted and Real Values for Training', datapoints,is_validation=False\n",
    "            )\n",
    "\n",
    "        for batch in range(len(self.predicted_values_val[0])):\n",
    "            self.plot_batch_predicted_and_real(\n",
    "                batch, self.predicted_values_val, self.real_values_val,\n",
    "                save_dir, f'Batch {batch} Predicted and Real Values for Validation',datapoints, is_validation=True\n",
    "            )\n",
    "\n",
    "#         # Transpose the abs_diff_history to get per-label data\n",
    "#         avg_abs_diff_history_train = np.array(abs_diff_history_train).T\n",
    "#         avg_abs_diff_history_val = np.array(abs_diff_history_val).T\n",
    "\n",
    "#         self.wout_avg_train.append(np.mean(avg_abs_diff_history_train))\n",
    "#         self.wout_avg_val.append(np.mean(avg_abs_diff_history_val))\n",
    "\n",
    "#         # Plot values for different iterations\n",
    "#         self.plot_values_for_iterations(\n",
    "#             self.wout_avg_train, abs_diff_history_train,\n",
    "#             save_dir, 'Values for Different Iterations for Training', is_validation=False\n",
    "#         )\n",
    "\n",
    "#         self.plot_values_for_iterations(\n",
    "#             self.wout_avg_val, abs_diff_history_val,\n",
    "#             save_dir, 'Values for Different Iterations for Validation', is_validation=True\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "80/80 [==============================] - 174s 2s/step - loss: 0.0417 - mean_absolute_error: 0.1576 - mean_squared_error: 0.0417 - val_loss: 0.0408 - val_mean_absolute_error: 0.1503 - val_mean_squared_error: 0.0409\n",
      "Epoch 2/15\n",
      "80/80 [==============================] - 170s 2s/step - loss: 0.0370 - mean_absolute_error: 0.1477 - mean_squared_error: 0.0369 - val_loss: 0.0390 - val_mean_absolute_error: 0.1502 - val_mean_squared_error: 0.0390\n",
      "Epoch 3/15\n",
      "80/80 [==============================] - 170s 2s/step - loss: 0.0358 - mean_absolute_error: 0.1463 - mean_squared_error: 0.0358 - val_loss: 0.0402 - val_mean_absolute_error: 0.1487 - val_mean_squared_error: 0.0403\n",
      "Epoch 4/15\n",
      "80/80 [==============================] - 174s 2s/step - loss: 0.0352 - mean_absolute_error: 0.1442 - mean_squared_error: 0.0352 - val_loss: 0.0372 - val_mean_absolute_error: 0.1465 - val_mean_squared_error: 0.0373\n",
      "Epoch 5/15\n",
      "80/80 [==============================] - 172s 2s/step - loss: 0.0350 - mean_absolute_error: 0.1439 - mean_squared_error: 0.0350 - val_loss: 0.0382 - val_mean_absolute_error: 0.1474 - val_mean_squared_error: 0.0382\n",
      "Epoch 6/15\n",
      "80/80 [==============================] - 176s 2s/step - loss: 0.0342 - mean_absolute_error: 0.1430 - mean_squared_error: 0.0342 - val_loss: 0.0363 - val_mean_absolute_error: 0.1472 - val_mean_squared_error: 0.0364\n",
      "Epoch 7/15\n",
      "80/80 [==============================] - 179s 2s/step - loss: 0.0331 - mean_absolute_error: 0.1409 - mean_squared_error: 0.0331 - val_loss: 0.0397 - val_mean_absolute_error: 0.1493 - val_mean_squared_error: 0.0397\n",
      "Epoch 8/15\n",
      "80/80 [==============================] - 178s 2s/step - loss: 0.0327 - mean_absolute_error: 0.1399 - mean_squared_error: 0.0327 - val_loss: 0.0356 - val_mean_absolute_error: 0.1457 - val_mean_squared_error: 0.0357\n",
      "Epoch 9/15\n",
      "79/80 [============================>.] - ETA: 0s - loss: 0.0325 - mean_absolute_error: 0.1383 - mean_squared_error: 0.0324"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Launch the training\n",
    "t0 = time.time()\n",
    "\n",
    "# Create the model and compile it\n",
    "model, base_model = model_utils.create_model(CONF, base_model=model_wrap.classify_model)\n",
    "\n",
    "# Get a list of the top layer variables that should not be applied a lr_multiplier\n",
    "base_vars = [var.name for var in base_model.trainable_variables]\n",
    "all_vars = [var.name for var in model.trainable_variables]\n",
    "top_vars = set(all_vars) - set(base_vars)\n",
    "top_vars = list(top_vars)\n",
    "\n",
    "# Set trainable layers\n",
    "if CONF['training']['mode'] == 'fast':\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "model.compile(optimizer=customAdam(lr=CONF['training']['initial_lr'],\n",
    "                                   amsgrad=True,\n",
    "                                   lr_mult=0.1,\n",
    "                                   excluded_vars=top_vars\n",
    "                                   ),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae', 'mse'])\n",
    "\n",
    "\n",
    "\n",
    "line_plot_callback = LinePlotCallback(train_gen,val_gen, val_steps, class_names)  # Initialize the callback\n",
    "\n",
    "# Include the custom callback in your list of callbacks\n",
    "callbacks = utils.get_callbacks(CONF)\n",
    "# callbacks.append(bar_plot_callback)\n",
    "callbacks.append(line_plot_callback)\n",
    "\n",
    "history = model.fit_generator(generator=train_gen,\n",
    "                              steps_per_epoch=train_steps,\n",
    "                              epochs=CONF['training']['epochs'],\n",
    "                              class_weight=None,\n",
    "                              validation_data=val_gen,\n",
    "                              validation_steps=val_steps,\n",
    "                              callbacks=callbacks,\n",
    "                              verbose=1, max_queue_size=5, workers=4,\n",
    "                              use_multiprocessing=CONF['training']['use_multiprocessing'],\n",
    "                              initial_epoch=0)\n",
    "\n",
    "\n",
    "# Saving everything\n",
    "print('Saving data to {} folder.'.format(paths.get_timestamped_dir()))\n",
    "print('Saving training stats ...')\n",
    "stats = {'epoch': history.epoch,\n",
    "         'training time (s)': round(time.time()-t0, 2),\n",
    "         'timestamp': TIMESTAMP}\n",
    "stats.update(history.history)\n",
    "stats = json_friendly(stats)\n",
    "stats_dir = paths.get_stats_dir()\n",
    "with open(os.path.join(stats_dir, 'stats.json'), 'w') as outfile:\n",
    "    json.dump(stats, outfile, sort_keys=True, indent=4)\n",
    "\n",
    "print('Saving the configuration ...')\n",
    "model_utils.save_conf(CONF)\n",
    "\n",
    "print('Saving the model to h5...')\n",
    "fpath = os.path.join(paths.get_checkpoints_dir(), 'final_model.h5')\n",
    "model.save(fpath)\n",
    "\n",
    "# print('Saving the model to protobuf...')\n",
    "# fpath = os.path.join(paths.get_checkpoints_dir(), 'final_model.proto')\n",
    "# model_utils.save_to_pb(model, fpath)\n",
    "\n",
    "print('Finished')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Plot boxplots for each label\n",
    "\n",
    "plt.boxplot(y_train)\n",
    "plt.title(f'Boxplot for')\n",
    "plt.ylim(-0.1, 1)  # Set y-axis limit from 0 to 1\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.tight_layout()  # Ensure proper spacing\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Plot boxplots for each label\n",
    "\n",
    "plt.boxplot(y_val)\n",
    "plt.title(f'Boxplot for')\n",
    "plt.ylim(-0.1, 1)  # Set y-axis limit from 0 to 1\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.tight_layout()  # Ensure proper spacing\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.562, 0.046, 0.084, ..., 0.605, 0.574, 0.777], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
